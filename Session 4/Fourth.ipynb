{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_4_EVA_Fourth.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nrajmalwar/Project/blob/master/Session%204/Fourth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLaS9qHarzVC",
        "colab_type": "text"
      },
      "source": [
        "Nishad Rajmalwar, Batch F6, nrajmalwar@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cunfvxU8MbD8",
        "colab_type": "text"
      },
      "source": [
        "###Planning and Expectations###\n",
        "\n",
        "1.   Our aim is to reach above 99.4% accuracy in this model.\n",
        "2. We also want our model to train faster, hence we use a bigger batch size. But a bigger batch size also requires tuning the learning rate. We try several values of the learning to suit our higher batch size.\n",
        "3. We use LRscheduler to change the value of learning rate with each epoch. We decrease it by a factor of some percentage of the previous epoch value.\n",
        "4. We experiment with different optimizers like Adam or SGD to find better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "# **Import Libraries and modules**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2yrkAO1r4n5",
        "colab_type": "text"
      },
      "source": [
        "Install and import keras library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "outputId": "468cdc7a-b78a-4c4a-8766-6ee09066ad29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZixXfX3r7PO",
        "colab_type": "text"
      },
      "source": [
        "Important important APIs, functions and libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D, GlobalAveragePooling2D, SeparableConv2D\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.datasets import mnist\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTflyJG_sBDu",
        "colab_type": "text"
      },
      "source": [
        "Load and split the data into training and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hvs3DaXsGzv",
        "colab_type": "text"
      },
      "source": [
        "Print the shape of X_train (examples of the dataset) and plot the first image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "f09a421d-b0af-4a16-f1cc-ca9310a214e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[108])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f32169ac0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUYLvowQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfF0dCyfsSZ4",
        "colab_type": "text"
      },
      "source": [
        "Reshape the X_train and X_test to (m,  28, 28, 1) tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZBh78EBsZdX",
        "colab_type": "text"
      },
      "source": [
        "Change the values of X_train, X_test to float32 and normalize them between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iawmitd5sftJ",
        "colab_type": "text"
      },
      "source": [
        "Print the first 10 elements of y_train which contains the class labels for X_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "741ae937-2c7d-4db1-f271-23534f370019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khh_eB46szz2",
        "colab_type": "text"
      },
      "source": [
        "Convert the values of Y_train and Y_test to one-hot vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "b38ba864-e639-4881-c391-257b37d4e2ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "Y_train[:10]\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZAZ_CIXzqWQ",
        "colab_type": "text"
      },
      "source": [
        "#Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bUW3bHFs4sw",
        "colab_type": "text"
      },
      "source": [
        "Build a simple convolution model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKqT73Q9JJB",
        "colab_type": "code",
        "outputId": "a3c2affe-d3f0-48a8-b9e0-815e1883edb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "model = Sequential()\n",
        " \n",
        "model.add(Convolution2D(10, 3, 3, activation='relu', input_shape=(28,28,1))) # Output size = 26; Receptive Field = 3\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu')) # 24; 5\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(20, 3, 3, activation='relu')) # 22; 7\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) # 11; 14\n",
        "\n",
        "model.add(Convolution2D(10, 1, 1, activation='relu')) # 11; 16\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu')) # 9; 18\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu')) # 7; 20\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Convolution2D(20, 3, 3, activation='relu')) # 5; 22\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(10, 1, activation='relu')) \n",
        "model.add(Convolution2D(10, 5))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (1, 1), activation=\"relu\")`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YsOCVEGtDcN",
        "colab_type": "text"
      },
      "source": [
        "Print the model summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzdAYg1k9K7Z",
        "colab_type": "code",
        "outputId": "79e38604-2b0d-4678-b8cc-f8e826f01a5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_18 (Conv2D)           (None, 26, 26, 10)        100       \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 26, 26, 10)        40        \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 24, 24, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 22, 22, 20)        2900      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 22, 22, 20)        80        \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 22, 22, 20)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 11, 11, 20)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 11, 11, 10)        210       \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 11, 11, 10)        40        \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 9, 9, 16)          1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 5, 5, 20)          2900      \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 5, 5, 20)          80        \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 5, 5, 10)          210       \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 1, 1, 10)          2510      \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,494\n",
            "Trainable params: 14,278\n",
            "Non-trainable params: 216\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6EKlf6ptE_O",
        "colab_type": "text"
      },
      "source": [
        "Compile the model with the loss function, the optimizer and the metrics used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsXKClA3JCHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YCwPhFVzwdD",
        "colab_type": "text"
      },
      "source": [
        "#Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfeVPqhttK3u",
        "colab_type": "text"
      },
      "source": [
        "Fit the model on X_train and Y_train for 10 epochs with a batch size of 32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4BHAyTXg_7L",
        "colab_type": "code",
        "outputId": "88d65156-fe03-4d6e-f822-9366822eae1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2132
        }
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=128, epochs=30, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.2032 - acc: 0.9350 - val_loss: 0.1033 - val_acc: 0.9709\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0563 - acc: 0.9827 - val_loss: 0.0505 - val_acc: 0.9837\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0419 - acc: 0.9868 - val_loss: 0.0493 - val_acc: 0.9858\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0344 - acc: 0.9892 - val_loss: 0.0382 - val_acc: 0.9888\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0297 - acc: 0.9904 - val_loss: 0.0401 - val_acc: 0.9882\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0275 - acc: 0.9909 - val_loss: 0.0349 - val_acc: 0.9896\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0253 - acc: 0.9919 - val_loss: 0.0341 - val_acc: 0.9897\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0230 - acc: 0.9929 - val_loss: 0.0317 - val_acc: 0.9915\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0208 - acc: 0.9936 - val_loss: 0.0291 - val_acc: 0.9910\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0192 - acc: 0.9937 - val_loss: 0.0318 - val_acc: 0.9916\n",
            "Epoch 11/30\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0176 - acc: 0.9944 - val_loss: 0.0327 - val_acc: 0.9912\n",
            "Epoch 12/30\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0168 - acc: 0.9947 - val_loss: 0.0362 - val_acc: 0.9903\n",
            "Epoch 13/30\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0171 - acc: 0.9945 - val_loss: 0.0280 - val_acc: 0.9921\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0152 - acc: 0.9948 - val_loss: 0.0251 - val_acc: 0.9935\n",
            "Epoch 15/30\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0133 - acc: 0.9959 - val_loss: 0.0254 - val_acc: 0.9927\n",
            "Epoch 16/30\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0136 - acc: 0.9955 - val_loss: 0.0299 - val_acc: 0.9920\n",
            "Epoch 17/30\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0127 - acc: 0.9958 - val_loss: 0.0239 - val_acc: 0.9933\n",
            "Epoch 18/30\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 0.0116 - acc: 0.9961 - val_loss: 0.0310 - val_acc: 0.9916\n",
            "Epoch 19/30\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0117 - acc: 0.9962 - val_loss: 0.0334 - val_acc: 0.9914\n",
            "Epoch 20/30\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0114 - acc: 0.9961 - val_loss: 0.0280 - val_acc: 0.9919\n",
            "Epoch 21/30\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0099 - acc: 0.9967 - val_loss: 0.0327 - val_acc: 0.9924\n",
            "Epoch 22/30\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0103 - acc: 0.9966 - val_loss: 0.0286 - val_acc: 0.9924\n",
            "Epoch 23/30\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.0092 - acc: 0.9972 - val_loss: 0.0327 - val_acc: 0.9918\n",
            "Epoch 24/30\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0094 - acc: 0.9968 - val_loss: 0.0299 - val_acc: 0.9914\n",
            "Epoch 25/30\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0092 - acc: 0.9970 - val_loss: 0.0249 - val_acc: 0.9941\n",
            "Epoch 26/30\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0085 - acc: 0.9972 - val_loss: 0.0265 - val_acc: 0.9935\n",
            "Epoch 27/30\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0087 - acc: 0.9971 - val_loss: 0.0301 - val_acc: 0.9933\n",
            "Epoch 28/30\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0083 - acc: 0.9971 - val_loss: 0.0339 - val_acc: 0.9921\n",
            "Epoch 29/30\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0079 - acc: 0.9973 - val_loss: 0.0351 - val_acc: 0.9921\n",
            "Epoch 30/30\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0069 - acc: 0.9979 - val_loss: 0.0301 - val_acc: 0.9934\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f31b4b9f898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDsFQbCpzzhV",
        "colab_type": "text"
      },
      "source": [
        "#Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6mz9k6wtQCq",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsH-lLk-eLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsPC3p8utTiq",
        "colab_type": "text"
      },
      "source": [
        "Print the evaluation score of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkX8JMv79q9r",
        "colab_type": "code",
        "outputId": "b54a7268-e3f0-4301-8a10-d3e6b76061b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.030127466949369228, 0.9934]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcaRgtkOz1x7",
        "colab_type": "text"
      },
      "source": [
        "#Model Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-omd0xUhtWBs",
        "colab_type": "text"
      },
      "source": [
        "Use the model to predict the class labels of test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxQKMo9ntbOR",
        "colab_type": "text"
      },
      "source": [
        "Compare the predicted values and the actual values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "outputId": "b5e7cfce-70b8-4d98-dac2-79fb2250b4df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(y_test[:9])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.14254814e-10 7.75933131e-08 2.12016388e-07 9.98408538e-08\n",
            "  2.32986563e-13 6.14065909e-10 7.17573755e-17 9.99999642e-01\n",
            "  3.23628581e-12 1.29152522e-09]\n",
            " [2.32894331e-06 1.78988252e-04 9.99815643e-01 3.00066773e-07\n",
            "  3.43109207e-07 1.42385581e-09 2.09503037e-06 9.21276833e-09\n",
            "  2.16923567e-07 2.49902959e-10]\n",
            " [1.68343455e-08 9.99996781e-01 1.18230910e-08 4.80276041e-09\n",
            "  4.00783136e-07 1.56326465e-07 1.59692263e-07 2.42549891e-06\n",
            "  4.31139746e-08 1.27029480e-07]\n",
            " [9.99976516e-01 1.56706506e-08 1.14408058e-05 9.34513800e-09\n",
            "  3.03197396e-08 8.63075229e-08 1.07154337e-05 2.50079317e-08\n",
            "  4.77361404e-08 1.19856134e-06]\n",
            " [6.96602314e-08 1.37526149e-05 2.37304093e-07 9.46317025e-10\n",
            "  9.99949455e-01 2.67112665e-09 3.19970139e-09 1.38210507e-05\n",
            "  4.71867123e-09 2.26952980e-05]\n",
            " [3.97485458e-08 9.99993086e-01 3.95063751e-08 4.76623985e-09\n",
            "  6.76039519e-07 1.58762745e-08 4.43472743e-08 6.05280820e-06\n",
            "  3.16506146e-08 1.13516030e-07]\n",
            " [5.66494975e-08 7.03865022e-04 6.73631803e-05 3.04958718e-07\n",
            "  9.97325182e-01 1.01469805e-05 1.30859057e-08 1.79737702e-03\n",
            "  4.06756481e-05 5.50855548e-05]\n",
            " [1.70933532e-07 1.84874338e-07 1.20583343e-06 2.79950518e-06\n",
            "  2.13892017e-05 6.70303707e-05 3.28085448e-09 1.47989541e-08\n",
            "  1.76622798e-05 9.99889493e-01]\n",
            " [6.67200595e-07 3.51923779e-09 6.10420559e-09 5.82653513e-07\n",
            "  1.11611475e-11 9.99995947e-01 2.34526510e-06 4.19606145e-08\n",
            "  2.62325671e-07 5.31931370e-08]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49wgsnzKRx7m",
        "colab_type": "text"
      },
      "source": [
        "###Results###\n",
        "1. Our model reaches 99.41 validation accuracy by the 25th epoch. Our target is reached. We expected our model to perform even better but it did not. However, the model reaches accuracy of above 99.3 more often, which means this model is more stable to achieve a higher accuracy consistently. Also, now each epoch is just 6 seconds compared to the previous time of 39 seconds.\n",
        "2. We tried using a initial learning rate of 0.003 and 0.001, and 0.003 works better.\n",
        "3. We tried several batch sizes from 32, 64, 128, 256 to 512. This model performs good for 64 batch size but takes around 20 seconds to complete each epoch. With 256 and 512, the model trains very fast, just 3-4 seconds per epoch, however it starts overfitting on the training set and does not reach the target accuracy. We also tried reducing the learning rate for these batch sizes, but there was no improvement. The batch size of 128 works best in terms of performance and time taken.\n",
        "4. We try similar experminents with SGD optimizer but the initial few epochs itself start with very low accuracy compared to Adam, so we can conclude that Adam works better in this case."
      ]
    }
  ]
}