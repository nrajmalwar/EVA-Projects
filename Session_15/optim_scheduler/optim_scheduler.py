# -*- coding: utf-8 -*-
"""optim_scheduler.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LJS8DcfRO8juwBNeESdsqvfuX0RQ1a0k
"""

import numpy as np
import tensorflow as tf

def lr_schedule(t, epochs, learning_rate):
  """Define the interpolated values for learning rate"""
  return np.interp([t], [0, (epochs+1)//5, epochs], [0, learning_rate, 0])[0]

  # return lr_sch

def lr_func(len_train, batch_size, epochs, learning_rate, global_step):
  """Define the learning rate function"""
  # global_step = tf.train.get_or_create_global_step()
  batches_per_epoch = len_train//batch_size + 1
  return lr_schedule(global_step/batches_per_epoch, epochs, learning_rate)/batch_size

  # return lr_function

def momentum_optimizer(lr_func, lr_momentum):
  """Define the optimiser function"""
  opt = tf.train.MomentumOptimizer(lr_func, momentum=lr_momentum, use_nesterov=True)

  return opt

def data_aug(x, y):
  """Apply data augmentations"""
  return tf.image.random_flip_left_right(tf.random_crop(x, [32, 32, 3])), y

  # return aug
