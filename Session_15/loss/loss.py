# -*- coding: utf-8 -*-
"""loss.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L2uoDXg2032xAA3OHQcMAQJZT7z2IjEI
"""

import numpy as np
import time, math
import tensorflow as tf
import tensorflow.contrib.eager as tfe
import matplotlib.pyplot as plt
from tqdm import tqdm_notebook as tqdm

def model_training(model, original_train_set, test_set, EPOCHS, LEARNING_RATE, BATCH_SIZE, WEIGHT_DECAY, opt, global_step, len_train, lr_schedule, data_aug):
  """Train the model"""
  t = time.time()

  list_train_acc = []
  list_test_acc = []
  list_train_loss = []
  list_test_loss = []

  for epoch in range(EPOCHS):
    train_loss = test_loss = train_acc = test_acc = 0.0

    train_set = original_train_set.map(data_aug).map(data_aug).shuffle(len_train).batch(BATCH_SIZE).prefetch(1)

    tf.keras.backend.set_learning_phase(1)
    for (x, y) in tqdm(train_set):
      with tf.GradientTape() as tape:
        loss, correct = model(x, y)

      var = model.trainable_variables
      grads = tape.gradient(loss, var)
      for g, v in zip(grads, var):
        g += v * WEIGHT_DECAY * BATCH_SIZE
      opt.apply_gradients(zip(grads, var), global_step=global_step)

      train_loss += loss.numpy()
      train_acc += correct.numpy()

    tf.keras.backend.set_learning_phase(0)
    for (x, y) in test_set:
      loss, correct = model(x, y)

      test_loss += loss.numpy()
      test_acc += correct.numpy()

    list_test_loss.append(test_loss)
    list_test_acc.append(test_acc)
    list_train_loss.append(train_loss)
    list_train_acc.append(train_acc)

    print('epoch:', epoch+1, 'lr:', lr_schedule(epoch+1, EPOCHS, LEARNING_RATE), 'train loss:', train_loss / len_train, 'train acc:', train_acc / len_train, 'val loss:', test_loss / len_test, 'val acc:', test_acc / len_test, 'time:', time.time() - t)

  return list_train_acc, list_test_acc, list_train_loss, list_test_loss
